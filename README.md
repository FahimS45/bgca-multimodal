# BGCA-Fusion: Bidirectional Gated Cross-Attention for Multimodal Breast Cancer Diagnosis from Mammograms and Clinical Reports

This repository contains the codebase for our research paper, "BGCA-Fusion: Bidirectional Gated Cross-Attention for Multimodal Breast Cancer Diagnosis from Mammograms and Clinical Reports." 

## Abstract
Breast cancer remains the second leading cause of cancer fatalities in women, and while AI excels in unimodal learning, effective diagnosis often demands a multimodal perspective of patient data. This study proposes an ideal multimodal methodology that integrates breast medical images with natural-text reports derived from structured clinical tables to classify non-malignant or malignant cases confidently. For image data, five Convolutional Neural Networks (CNNs) and two Vision Transformers (ViTs) were evaluated with and without extensive data optimization techniques. The best-performing models were used for vision feature extraction and combined through an ensemble approach. Transformed textual data were then processed using four state-of-the-art Large Language Models (LLMs) and fused with the vision features through a Bidirectional Gated Cross-Attention (BGCA) mechanism. To the best of our knowledge, we are the first to explore metadata transformation into natural, context-specific sentences within the breast cancer domain. Evaluation was conducted on three mammography datasets: DMID, INbreast, and MIAS, across ten random seeds. The proposed method consistently outperformed unimodal baselines, achieving up to 96.36\% accuracy and 95.38\% macro F1 on DMID, along with a 99.74\% AUC on INbreast. When tested on the combined distribution, 94.54\% macro F1, and 98.90\% AUC were observed. Comprehensive ablation studies, statistical testing, and cross-dataset validation confirmed the robustness of our framework. In a semi-supervised domain adaptation setting, incorporating only 10\% of the target domain data led to an 8.7\% improvement in F1-score. Furthermore, model predictions were interpreted using Grad-CAM and SHAP to enhance transparency. The full codebase is available on GitHub.

## System Architecture
The core of our methodology is a multimodal learning framework that effectively combines visual and textual features for enhanced diagnostic accuracy. The system architecture, as detailed in the diagram below, outlines the key stages of our approach:
![System Architecture](assets/Final_architecture.png)

